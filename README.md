# üìò TP BIG DATA - Cluster Spark avec Docker, Hadoop, YARN et MongoDB Atlas

## üìã Description
TP r√©alis√© dans le cadre du cours de Big Data (Ann√©e Universitaire 2025-2026). 
Ce TP a pour objectif de mettre en place un cluster Big Data complet avec Docker, incluant Hadoop HDFS, YARN, Spark, et d'effectuer des analyses de donn√©es avec PySpark sur Google Colab avec int√©gration MongoDB Atlas.

## üéØ Objectifs du TP
- ‚ùñ Installer un cluster Spark avec Docker
- ‚ùñ Ex√©cuter des premiers exemples sur Apache Spark
- ‚ùñ Installer PySpark sur Google Colab
- ‚ùñ Charger et manipuler des donn√©es avec Spark
- ‚ùñ √âtude de cas : Int√©gration de Spark avec MongoDB Atlas
- ‚ùñ Visualisation des r√©sultats avec Matplotlib et Seaborn

---  

## üìÅ Structure du Projet :
```
BIGDATA_LAB_cluster_spark/
‚îú‚îÄ‚îÄ README.md                           # Documentation principale
‚îú‚îÄ‚îÄ screenshots/                        # Captures d'√©cran des interfaces
‚îÇ   ‚îú‚îÄ‚îÄ hadoop/hadoop.png              # Interface Hadoop HDFS
‚îÇ   ‚îú‚îÄ‚îÄ yarn/Yarn.png                  # Interface YARN ResourceManager
‚îÇ   ‚îî‚îÄ‚îÄ spark/Spark.png                # Interface Spark Master
‚îú‚îÄ‚îÄ colab_notebooks/                    # Notebooks Google Colab
‚îÇ   ‚îî‚îÄ‚îÄ TP_Cluster_spark_colab.ipynb   # Notebook principal avec analyses
‚îú‚îÄ‚îÄ docker_config/                      # Fichiers de configuration Docker
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml             # Configuration du cluster
‚îÇ   ‚îú‚îÄ‚îÄ spark-defaults.conf            # Configuration Spark
‚îÇ   ‚îî‚îÄ‚îÄ start-scripts/                 # Scripts de d√©marrage
‚îú‚îÄ‚îÄ data/                               # Jeux de donn√©es utilis√©s
‚îÇ   ‚îî‚îÄ‚îÄ transactions.csv               # Donn√©es de transactions financi√®res
‚îú‚îÄ‚îÄ scripts/                            # Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ start-cluster.sh               # Script de d√©marrage du cluster
‚îÇ   ‚îî‚îÄ‚îÄ submit-jobs.sh                 # Soumission de jobs Spark
‚îú‚îÄ‚îÄ examples/                           # Exemples de code
‚îÇ   ‚îú‚îÄ‚îÄ wordcount.py                   # Exemple WordCount Python
‚îÇ   ‚îú‚îÄ‚îÄ sparkpi.py                     # Calcul de Pi avec Spark
‚îÇ   ‚îî‚îÄ‚îÄ mongodb-integration.py         # Int√©gration MongoDB
‚îî‚îÄ‚îÄ documentation/                      # Documentation compl√©mentaire
    ‚îî‚îÄ‚îÄ lab_cluster_spark_25-26.pdf    # √ânonc√© du TP
```

---

## üì∏ Captures d'√©cran des Interfaces

### 1. Interface Hadoop HDFS NameNode
![Hadoop HDFS Interface](screenshots/hadoop/hadoop.png)
*Interface web du NameNode montrant l'√©tat du syst√®me de fichiers distribu√© HDFS*

### 2. Interface YARN ResourceManager
![YARN ResourceManager Interface](screenshots/yarn/Yarn.png)
*YARN g√©rant l'allocation des ressources CPU et m√©moire sur le cluster*

### 3. Interface Spark Master
![Spark Master Interface](screenshots/spark/Spark.png)
*Spark Master avec les workers connect√©s et les applications en cours d'ex√©cution*

---

## üî¨ Notebook Google Colab - Analyses PySpark avec MongoDB


### üìì Contenu du Notebook Principal : `TP_Cluster_spark_colab.ipynb`

**Partie 1 : Installation et Configuration**
1. Installation d'Apache Spark et PySpark sur Colab
2. Configuration des variables d'environnement (JAVA_HOME, SPARK_HOME)
3. D√©marrage d'une session Spark

**Partie 2 : Premiers Exemples avec Spark**
- Cr√©ation de DataFrames simples
- Op√©rations de base (s√©lection, filtrage, sch√©ma)
- V√©rification de la version de Spark

**Partie 3 : Analyse de Donn√©es de Transactions Financi√®res**
- Chargement de fichiers CSV
- Exploration et manipulation des donn√©es
- Filtrage des transactions sup√©rieures √† 1000‚Ç¨
- Calcul du montant total par type de transaction
- Tri des transactions par montant d√©croissant

**Partie 4 : Int√©gration avec MongoDB Atlas**
- Installation du connecteur MongoDB Spark
- Configuration de la connexion √† MongoDB Atlas
- Chargement des transactions depuis MongoDB
- Analyses avanc√©es avec les donn√©es MongoDB
- Utilisation de Spark SQL pour les requ√™tes

**Partie 5 : Visualisation des R√©sultats**
- Graphique barplot : Montant total des transactions par type
- Histogramme : Distribution des montants des transactions
- Comparaison des transactions r√©ussies vs √©chou√©es
- Visualisations avec Seaborn et Matplotlib

---

## üèóÔ∏è Architecture du Cluster D√©ploy√©

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Cluster Docker (Local)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  Hadoop    ‚îÇ  ‚îÇ    YARN    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  NameNode  ‚îÇ  ‚îÇ Resource   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  (HDFS)    ‚îÇ  ‚îÇ  Manager   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   :9870    ‚îÇ  ‚îÇ   :8088    ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Spark     ‚îÇ ‚îÇ  Spark     ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Master    ‚îÇ ‚îÇ  Workers   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   :8080     ‚îÇ ‚îÇ (x2)       ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Google Colab (Cloud)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  Notebook PySpark           ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  - Analyse de donn√©es       ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  - Connexion MongoDB Atlas  ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  - Visualisations           ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        MongoDB Atlas (Cloud)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  Base de donn√©es            ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  bankdb.transactions        ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  Collections NoSQL          ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Travail R√©alis√©

### Phase 1 : Installation du Cluster avec Docker
- **Docker Compose** : Configuration compl√®te des services
- **Services d√©ploy√©s** :
  - Hadoop NameNode (port 9870)
  - YARN ResourceManager (port 8088)
  - Spark Master (port 8080)
  - 2x Spark Workers / YARN NodeManagers
  - 2x Hadoop DataNodes
- **Scripts de d√©marrage** : `start-hadoop.sh`, `start-spark.sh`

### Phase 2 : Configuration et Tests
- **Hadoop HDFS** : Configuration avec r√©plication (facteur 2)
- **YARN** : Allocation des ressources (m√©moire, CPU)
- **Spark** : Int√©gration avec YARN comme cluster manager
- **Tests de fonctionnement** : Acc√®s aux interfaces web, soumission de jobs

### Phase 3 : Premiers Exemples Spark
1. **SparkPi** : Calcul de œÄ avec diff√©rentes valeurs
   ```bash
   spark-submit --class org.apache.spark.examples.SparkPi \
                --master local[*] \
                $SPARK_HOME/examples/jars/spark-examples_{version}.jar 100
   ```

2. **WordCount en Scala** : Comptage de mots dans un fichier texte
   ```scala
   val data = sc.textFile("hdfs://hadoop-master:9000/user/root/input/alice.txt")
   val count = data.flatMap(line => line.split(" "))
                   .map(word => (word, 1))
                   .reduceByKey(_+_)
   count.saveAsTextFile("hdfs://hadoop-master:9000/user/root/output/respark")
   ```

3. **WordCount en Python** : Version Python du comptage de mots
   ```python
   spark = SparkSession.builder.master("yarn").appName('wordcount').getOrCreate()
   data = spark.sparkContext.textFile("hdfs://hadoop-master:9000/user/root/input/alice.txt")
   words = data.flatMap(lambda line: line.split(" "))
   wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)
   wordCounts.saveAsTextFile("hdfs://hadoop-master:9000/user/root/output/rr2")
   ```

### Phase 4 : Installation PySpark sur Google Colab
- **Installation des d√©pendances** : Java 8, Spark 3.2.1, PySpark
- **Configuration environnement** : Variables JAVA_HOME, SPARK_HOME
- **Initialisation Spark** : Utilisation de findspark pour l'initialisation
- **Session Spark** : Cr√©ation d'une session avec configuration m√©moire

### Phase 5 : Analyse de Donn√©es avec Spark
- **Chargement CSV** : Donn√©es de transactions financi√®res
- **Manipulation DataFrames** : Filtrage, regroupement, tri
- **Transformations** : Op√©rations sur les colonnes, agr√©gations
- **Sch√©ma** : Analyse de la structure des donn√©es

### Phase 6 : Int√©gration MongoDB Atlas (√âtude de Cas)
- **Connecteur MongoDB Spark** : Installation et configuration
- **Connexion √† MongoDB Atlas** : URI de connexion s√©curis√©e
- **Chargement des donn√©es** : Lecture des collections MongoDB dans Spark
- **Analyses avec donn√©es MongoDB** :
  - Calcul du montant moyen des transactions par type
  - Identification des comptes avec plus de 5 transactions
  - Agr√©gations complexes avec Spark SQL
- **Configuration de s√©curit√©** : Gestion des identifiants et permissions

### Phase 7 : Visualisation des R√©sultats
- **Graphiques avec Seaborn** : Barplots, histogrammes, comparaisons
- **Analyse statistique** : Distribution des montants, taux de r√©ussite
- **Export des r√©sultats** : Conversion Pandas pour visualisation
- **Dashboard** : Vue d'ensemble des transactions

### Phase 8 : Tests et Validation
- ‚úÖ Acc√®s aux interfaces web (Hadoop:9870, YARN:8088, Spark:8080)
- ‚úÖ Communication entre tous les services Docker
- ‚úÖ Soumission r√©ussie de jobs Spark (SparkPi, WordCount)
- ‚úÖ Connexion √† MongoDB Atlas depuis Spark
- ‚úÖ Lecture/√©criture de donn√©es dans MongoDB
- ‚úÖ Ex√©cution compl√®te du notebook sur Google Colab
- ‚úÖ G√©n√©ration des visualisations

---

## üìä Commandes Principales Ex√©cut√©es

### 1. Acc√®s et D√©marrage du Cluster
```bash
# Acc√©der au conteneur master
docker exec -it hadoop-master bash

# D√©marrer Hadoop et YARN
./start-hadoop.sh
./start-spark.sh

# V√©rifier les services
jps
```

### 2. Interfaces Web
- **YARN Web UI** : https://localhost:8088
- **Spark Web UI** : https://localhost:8080
- **Hadoop HDFS UI** : https://localhost:9870

### 3. Soumission de Jobs Spark
```bash
# Exemple SparkPi
spark-submit --class org.apache.spark.examples.SparkPi \
             --master yarn \
             $SPARK_HOME/examples/jars/spark-examples_*.jar 100

# WordCount Python
spark-submit --master yarn wordcount.py
```

### 4. Installation sur Google Colab
```python
# Installation des d√©pendances
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark pyspark py4j pymongo matplotlib seaborn

# Configuration environnement
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"
import findspark
findspark.init()

# Session Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("ColabSpark") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()
```

### 5. Int√©gration MongoDB Atlas
```python
# Configuration connexion MongoDB
mongo_uri = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/bankdb.transactions?retryWrites=true&w=majority"

# Session Spark avec MongoDB
spark = SparkSession.builder \
    .appName("MongoDBIntegration") \
    .config("spark.mongodb.input.uri", mongo_uri) \
    .config("spark.mongodb.output.uri", mongo_uri) \
    .getOrCreate()

# Chargement des donn√©es depuis MongoDB
df_mongo = spark.read.format("mongo").option("uri", mongo_uri).load()
```

### 6. Arr√™t du Cluster
```bash
# Arr√™ter les conteneurs
docker stop hadoop-master hadoop-slave1 hadoop-slave2

# Ou utiliser docker-compose
docker-compose down
```

---

## üéì Comp√©tences Acquises

### Techniques
1. **Orchestration Docker** : Gestion de clusters multi-conteneurs
2. **Architecture Big Data** : Compr√©hension HDFS + YARN + Spark
3. **Spark Distributed Computing** : Traitement distribu√© de donn√©es
4. **PySpark Programming** : D√©veloppement d'applications Spark en Python
5. **MongoDB Integration** : Connexion Spark √† bases de donn√©es NoSQL
6. **Data Visualization** : Cr√©ation de graphiques avec Seaborn/Matplotlib
7. **Cloud Integration** : Utilisation de Google Colab et MongoDB Atlas

### Pratiques
- Configuration et optimisation de clusters Spark
- D√©bogage d'applications distribu√©es
- Gestion de la m√©moire et des ressources
- S√©curisation des connexions aux bases de donn√©es
- Automatisation des d√©ploiements avec Docker
- Analyse de performances des jobs Spark



 
 
